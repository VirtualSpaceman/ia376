{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Datasets import Modes, WikiTable, SquadDataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pergunta = \"what was the last year where this team was a part of the usl a-league?\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "# target = tokenizer.encode_plus(pergunta,\n",
    "#                                        padding='max_length',\n",
    "#                                        truncation=True,\n",
    "#                                        max_length=128,\n",
    "#                                        return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = T5EncoderModel.from_pretrained('t5-base')\n",
    "# decoder = decoder = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.config.d_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm2d(out_channels),\n",
    "                                   nn.LeakyReLU(),\n",
    "                                   nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(out_channels),\n",
    "                                   nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "embedding_extractor = nn.Sequential(ConvBlock(3, 16),\n",
    "                                     ConvBlock(16, 64),\n",
    "                                     ConvBlock(64, 256),\n",
    "                                     ConvBlock(256, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "ds = WikiTable(Modes.TRAIN, tokenizer=tokenizer, max_len=128, img_shape=(600, 1600))\n",
    "sample = ds[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table_img': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " 'question_ids': tensor([   84,    19,  7231,     6,  6957,     3,    17,    76,   172,    42,\n",
       "          6957,     3, 13878,     7,     3,    17,    76,   172,   521,    58,\n",
       "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'question_attn_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'question': 'which is deeper, lake tuz or lake palas tuzla?',\n",
       " 'answer': 'Lake Palas Tuzla',\n",
       " 'target_ids': tensor([ 2154, 14294,     7,  2740,   172,   521,     1,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 12.916511535644531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/lib/python3.6/site-packages/torch/autograd/__init__.py:127: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "0 - Decoded: ['..........', 'I have never seen such a thing. I have never seen such a thing.', '..........', '..........', '..........', '..........']\n",
      "0 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 12.28172779083252\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "1 - Decoded: ['- -----------------', 'Die wichtigsten Faktoren, die bei der Auswahl der richtigen Kandidaten für die Kandidat', 'Die ersten nderungen wurden im Jahr 2000 durchgeführt. nderungen wurden im Jahr', '- - - - - - - - - -', 'Die ersten nderungen wurden im Jahr 2006 durchgeführt. hat sich die', '..........']\n",
      "1 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 6.845726490020752\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "2 - Decoded: ['Die ersten nderungen wurden im Jahr 2000 durchgeführt. nderungen wurden im Jahr', 'Die ffentlichkeit hat sich mit der ffentlichkeit auseinandergesetzt. ffentlichkeit', 'Die ersten nderungen wurden im Jahr 2000 durchgeführt.', '- - - - - - - - - -', 'Die ffentlichkeit hat sich mit der ffentlichkeit auseinandergesetzt. ffentlichkeit', 'Die ersten nderungen wurden im Jahr 2000 durchgeführt.']\n",
      "2 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 5.097042560577393\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "3 - Decoded: ['Dabei handelt es sich um eine eigenständige Technologie, die es', '- - - - - - - - - -', 'Dabei handelt es sich um eine eigenständige Technologie, die es', '-es ist es möglich, die ffentlichkeit zu ermutigen', 'Dabei handelt es sich um eine eigenständige Technologie, die es', 'Dabei handelt es sich um eine eigenständige Technologie, die es']\n",
      "3 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 4.17888879776001\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "4 - Decoded: ['2', '-', '-', '-', '-', '-']\n",
      "4 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 2.254746913909912\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "5 - Decoded: ['Platinum', '3', 'Summer', 'more', '2', '-']\n",
      "5 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 1.28787100315094\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "6 - Decoded: ['Platinum', '3', 'Summer', 'less', '3', '']\n",
      "6 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 0.5362951159477234\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "7 - Decoded: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "7 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 0.13495121896266937\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "8 - Decoded: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "8 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 0.027683978900313377\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "9 - Decoded: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "9 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 0.029645545408129692\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "10 - Decoded: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "10 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 0.01682179979979992\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "11 - Decoded: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "11 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 0.007006581872701645\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "12 - Decoded: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "12 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 0.0030251506250351667\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n",
      "13 - Decoded: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "13 - Real: ['Platinum', '3', 'Summer', 'less', '3', 'Tokyo, Japan']\n",
      "feat.shape: torch.Size([6, 3928, 768])\n",
      "Vai pfv loss: 0.0012399073457345366\n",
      "Hidden shp: torch.Size([6, 3928, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ca1cf74bfc96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-ca1cf74bfc96>\u001b[0m in \u001b[0;36msample\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m                 outputs = decoder(decoder_input_ids=decoded_ids,\n\u001b[1;32m    117\u001b[0m                                   \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                                   return_dict=True)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;31m#                 outputs = decoder(decover_input_embeds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, head_mask, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1479\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1481\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1482\u001b[0m         )\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m             )\n\u001b[1;32m    933\u001b[0m             \u001b[0;31m# layer_outputs is a tuple with:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m             )\n\u001b[1;32m    661\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         )\n\u001b[1;32m    582\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    470\u001b[0m         )\n\u001b[1;32m    471\u001b[0m         value_states = project(\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_value_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         )\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mproject\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;31m# cross-attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;31m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_value_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def sample():\n",
    "    \n",
    "    decoder = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "#     encoder = T5EncoderModel.from_pretrained('t5-base')\n",
    "\n",
    "    encoder = decoder.get_encoder()\n",
    "    \n",
    "    dl = DataLoader(ds, batch_size=6, shuffle=True)\n",
    "    samples = next(iter(dl))\n",
    "#     sample = ds[5]\n",
    "    \n",
    "#     enc_out = encoder(input_ids=sample['question_ids'].unsqueeze(0),\n",
    "#                      attention_mask=sample['question_attn_mask'].unsqueeze(0))\n",
    "    \n",
    "#     hidden = enc_out.last_hidden_state\n",
    "    \n",
    "#     print(f\"hidden shp: {hidden.shape}\")\n",
    "    \n",
    "#     print(sample['question_ids'].shape, sample['question_attn_mask'].shape)\n",
    "    \n",
    "    #torch.Size([1, 768, 32, 32])\n",
    "#     features = embedding_extractor(ds[0]['table_img'].unsqueeze(0))\n",
    "    \n",
    "    #torch.Size([1, 32, 32, 768]) -> torch.Size([1, 32*32, 768])\n",
    "#     features = features.permute(0, 2, 3, 1).view(1, -1, decoder.config.d_model)\n",
    "    \n",
    "#     torch.Size([1, 1152, 768])\n",
    "#     features = torch.cat([features, hidden], dim =1)\n",
    "    \n",
    "#     print(f\"feat shape: {features.shape}\")\n",
    "    \n",
    "    #torch.Size([1, 768, 1152])\n",
    "#     features = features.permute(0, 2, 1)\n",
    "    \n",
    "#     linear = nn.Linear(1088, ds.max_len, bias=False)\n",
    "    #torch.Size([1, 768, 128])\n",
    "#     proj = linear(features)\n",
    "    \n",
    "    #hidden state\n",
    "    #torch.Size([1, 128, 728])\n",
    "#     proj = proj.permute(0, 2, 1)\n",
    "    \n",
    "    #batch size x seqlen x self.d_model\n",
    "#     print(f\"proj shape: {proj.shape}\")\n",
    "    \n",
    "    opt = optim.Adam(itertools.chain(encoder.parameters(), decoder.parameters(),\n",
    "                                     embedding_extractor.parameters()), lr=5e-4)\n",
    "    \n",
    "    for epo in range(50):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        \n",
    "        enc_out = encoder(input_ids=samples['question_ids'],\n",
    "                          attention_mask=samples['question_attn_mask'],\n",
    "                          output_attentions=True)\n",
    "        \n",
    "        hidden = enc_out.last_hidden_state\n",
    "#         print(f\"hiddn.shape: {hidden.shape}\")\n",
    "        proj = None\n",
    "        \n",
    "        B = hidden.size(0)\n",
    "        features = embedding_extractor(samples['table_img'])\n",
    "        features = features.permute(0, 2, 3, 1).view(B, -1, decoder.config.d_model)\n",
    "        features = torch.cat([features, hidden], dim =1)\n",
    "#         print(f\"feat.shape: {features.shape}\")\n",
    "#         proj = F.relu(proj)\n",
    "        \n",
    "#         print(f\"proj.shape {proj.shape}\")\n",
    "        \n",
    "        \n",
    "        if proj is None:\n",
    "            proj = features\n",
    "        loss = decoder(encoder_outputs=(proj, ), \n",
    "                       labels=samples['target_ids']).loss\n",
    "\n",
    "#         loss = decoder(inputs_embeds=proj, \n",
    "#                        labels=sample['target_ids'].unsqueeze(0)).loss\n",
    "        \n",
    "        print(f\"Vai pfv loss: {loss}\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            embedding_extractor.eval()\n",
    "        \n",
    "            \n",
    "            enc_out = encoder(input_ids=samples['question_ids'],\n",
    "                     attention_mask=samples['question_attn_mask'],\n",
    "                             output_attentions=True)\n",
    "        \n",
    "            encoder_hidden_states = enc_out.last_hidden_state\n",
    "            features = embedding_extractor(samples['table_img'])\n",
    "            features = features.permute(0, 2, 3, 1).view(B, -1, decoder.config.d_model)\n",
    "            features = torch.cat([features, encoder_hidden_states], dim =1)\n",
    "            \n",
    "#             print(features.shape)\n",
    "#             proj = linear(features)\n",
    "#             proj = F.relu(proj)\n",
    "#             print(f\"proj.shape: {proj.shape}\")\n",
    "#             #-------------------------------------------------\n",
    "            encoder_hidden_states = features\n",
    "#             print(f\"Hidden shp: {encoder_hidden_states.shape}\")\n",
    "            B = encoder_hidden_states.size(0)\n",
    "    \n",
    "            decoded_ids = torch.full((B, 1),\n",
    "                                 decoder.config.decoder_start_token_id,\n",
    "                                 dtype=torch.long)\n",
    "            \n",
    "            for step in range(20):\n",
    "                outputs = decoder(decoder_input_ids=decoded_ids,\n",
    "                                  encoder_outputs=(encoder_hidden_states,),\n",
    "                                  return_dict=True)\n",
    "#                 outputs = decoder(decover_input_embeds)\n",
    "                logits = outputs[\"logits\"]\n",
    "\n",
    "                next_token_logits = logits[:, -1, :]\n",
    "\n",
    "                # Greedy decoding\n",
    "                next_token_id = next_token_logits.argmax(1).unsqueeze(-1)\n",
    "\n",
    "                # Check if output is end of senquence for all batches\n",
    "                if torch.eq(next_token_id[:, -1], tokenizer.eos_token_id).all():\n",
    "                    break\n",
    "\n",
    "                # Concatenate past ids with new id, keeping batch dimension\n",
    "                decoded_ids = torch.cat([decoded_ids, next_token_id], dim=-1)\n",
    "            \n",
    "            print(f\"{epo} - Decoded: {tokenizer.batch_decode(decoded_ids, skip_special_tokens=True)}\")\n",
    "            print(f\"{epo} - Real: {samples['answer']}\")\n",
    "    \n",
    "    for idx, elem in enumerate(samples['table_img']):\n",
    "        img = np.transpose(elem.cpu().numpy(), (1, 2, 0))\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        print(f\"Question: {samples['question'][idx]}\")\n",
    "#     print(f\"Questions: {samples['question']}\")\n",
    "    del encoder, decoder, hidden, enc_out, features, linear, proj, opt\n",
    "\n",
    "sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### target.input_ids.shape, target.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = encoder(input_ids=target.input_ids, attention_mask=target.attention_mask, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2139,  0.1358,  0.2902,  ..., -0.5094, -0.5432, -0.0968],\n",
       "         [-0.2147, -0.0238, -0.1447,  ..., -0.2354, -0.1443, -0.3103],\n",
       "         [-0.0410,  0.0897, -0.3892,  ..., -0.2407, -0.1751, -0.6424],\n",
       "         ...,\n",
       "         [-0.4393,  0.1653, -0.1272,  ...,  0.0248,  0.2123,  0.0096],\n",
       "         [-0.4393,  0.1653, -0.1272,  ...,  0.0248,  0.2123,  0.0096],\n",
       "         [-0.4393,  0.1653, -0.1272,  ...,  0.0248,  0.2123,  0.0096]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1536, 16, 40]), torch.Size([1, 128, 768]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape, a.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.cat([v.view(1, -1),a.last_hidden_state.view(1, -1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1081344])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenizer.encode_plus(\"204\",\n",
    "                                       padding='max_length',\n",
    "                                       truncation=True,\n",
    "                                       max_length=128,\n",
    "                                       return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "decoder = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3917, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(encoder_outputs=(t.permute(0, 2, 1), ), labels=labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.config.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajustado = embedding_extractor(img_tensor.unsqueeze(0)).permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 63, 768])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajustado = ajustado.view(1, -1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajustado = ajustado.view(1, -1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1575, 768])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajustado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 1575])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajustado.permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nn.Linear(1575, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = l(ajustado.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/WikiTableQuestions/csv/200-csv/1.table'\n",
    "f = open(p, 'r')\n",
    "x = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = x.replace(\"\\t\", \" \\t \")\n",
    "# x = x.replace(\"\\\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "x = re.sub(\"\\s\\s+\" , \" \", x)\n",
    "x = x.replace(\"\\n\", \" \\n \")\n",
    "# x.replace(/  +/g, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"| Year | Title | Role | Notes | | 1995 | Polio Water | Diane | Short film | | 1996 | New York Crossing | Drummond | Television film | | 1997 | Lawn Dogs | Devon Stockard | | | 1999 | Pups | Rocky | | | 1999 | Notting Hill | 12-Year-Old Actress | | | 1999 | The Sixth Sense | Kyra Collins | | | 2000 | Paranoid | Theresa | | | 2000 | Skipped Parts | Maurey Pierce | | | 2000 | Frankie & Hazel | Francesca 'Frankie' Humphries | Television film | | 2001 | Lost and Delirious | Mary 'Mouse' Bedford | | | 2001 | Julie Johnson | Lisa Johnson | | | 2001 | Tart | Grace Bailey | | | 2002 | A Ring of Endless Light | Vicky Austin | Television film | | 2003 | Octane | Natasha 'Nat' Wilson | | | 2006 | The Oh in Ohio | Kristen Taylor | | | 2007 | Closing the Ring | Young Ethel Ann | | | 2007 | St Trinian's | JJ French | | | 2007 | Virgin Territory | Pampinea | | | 2008 | Assassination of a High School President | Francesca Fachini | | | 2009 | Walled In | Sam Walczak | | | 2009 | Homecoming | Shelby Mercer | | | 2010 | Don't Fade Away | Kat | | | 2011 | You and I | Lana | | | 2012 | Into the Dark | Sophia Monet | | | 2012 | Ben Banks | Amy | | | 2012 | Apartment 1303 3D | Lara Slate | | | 2012 | Cyberstalker | Aiden Ashley | Television film | | 2013 | Bhopal: A Prayer for Rain | Eva Gascon | | | 2013 | A Resurrection | Jessie | Also producer | | 2013 | L.A. Slasher | The Actress | | | 2013 | Gutsy Frog | Ms. Monica | Television film |</s>\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"| Year | Title | Role | Notes | \\n | 1995 | Polio Water | Diane | Short film | \\n | 1996 | New York Crossing | Drummond | Television film | \\n | 1997 | Lawn Dogs | Devon Stockard | | \\n | 1999 | Pups | Rocky | | \\n | 1999 | Notting Hill | 12-Year-Old Actress | | \\n | 1999 | The Sixth Sense | Kyra Collins | | \\n | 2000 | Paranoid | Theresa | | \\n | 2000 | Skipped Parts | Maurey Pierce | | \\n | 2000 | Frankie & Hazel | Francesca 'Frankie' Humphries | Television film | \\n | 2001 | Lost and Delirious | Mary 'Mouse' Bedford | | \\n | 2001 | Julie Johnson | Lisa Johnson | | \\n | 2001 | Tart | Grace Bailey | | \\n | 2002 | A Ring of Endless Light | Vicky Austin | Television film | \\n | 2003 | Octane | Natasha 'Nat' Wilson | | \\n | 2006 | The Oh in Ohio | Kristen Taylor | | \\n | 2007 | Closing the Ring | Young Ethel Ann | | \\n | 2007 | St Trinian's | JJ French | | \\n | 2007 | Virgin Territory | Pampinea | | \\n | 2008 | Assassination of a High School President | Francesca Fachini | | \\n | 2009 | Walled In | Sam Walczak | | \\n | 2009 | Homecoming | Shelby Mercer | | \\n | 2010 | Don't Fade Away | Kat | | \\n | 2011 | You and I | Lana | | \\n | 2012 | Into the Dark | Sophia Monet | | \\n | 2012 | Ben Banks | Amy | | \\n | 2012 | Apartment 1303 3D | Lara Slate | | \\n | 2012 | Cyberstalker | Aiden Ashley | Television film | \\n | 2013 | Bhopal: A Prayer for Rain | Eva Gascon | | \\n | 2013 | A Resurrection | Jessie | Also producer | \\n | 2013 | L.A. Slasher | The Actress | | \\n | 2013 | Gutsy Frog | Ms. Monica | Television film | \\n \""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
