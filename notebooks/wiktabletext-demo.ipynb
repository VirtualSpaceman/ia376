{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Datasets import Modes, WikiTableText\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pergunta = \"what was the last year where this team was a part of the usl a-league?\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "# target = tokenizer.encode_plus(pergunta,\n",
    "#                                        padding='max_length',\n",
    "#                                        truncation=True,\n",
    "#                                        max_length=128,\n",
    "#                                        return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = T5EncoderModel.from_pretrained('t5-base')\n",
    "# decoder = decoder = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.config.d_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "ds = WikiTableText(Modes.TRAIN, tokenizer=tokenizer, max_len=1024)\n",
    "sample = ds[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  822,    10,    84,  ...,  3180, 19563,     1]),\n",
       " 'input_attn_mask': tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       " 'question': 'which is deeper, lake tuz or lake palas tuzla?',\n",
       " 'answer': 'Lake Palas Tuzla',\n",
       " 'table_str': '| Name in English | Name in Turkish | Area (km2) | Depth | Location (districts and/or provinces) |\\n| Lake Van | Van Gölü | 3755\\xa0km2 | 451\\xa0m | Van, Bitlis |\\n| Lake Tuz | Tuz Gölü | 1500\\xa0km2 | 2\\xa0m | Aksaray, Ankara, Konya |\\n| Lake Beyşehir | Beyşehir Gölü | 656\\xa0km2 | 10\\xa0m | Beyşehir in Konya, Isparta |\\n| Lake Eğirdir | Eğirdir Gölü | 482\\xa0km2 | | Isparta |\\n| Lake İznik | İznik Gölü | 308\\xa0km2 | | İznik in Bursa, Yalova |\\n| Lake Burdur | Burdur Gölü | 200\\xa0km2 | | Burdur, Isparta |\\n| Lake Manyas | Manyas Gölü | 166\\xa0km2 | | Balıkesir |\\n| Lake Acıgöl | Acıgöl | 153\\xa0km2 | | Denizli, Afyonkarahisar |\\n| Lake Uluabat | Uluabat Gölü | 134\\xa0km2 | 1–2\\xa0m | Bursa |\\n| Lake Çıldır | Çıldır Gölü | 115\\xa0km2 | | Ardahan, Kars |\\n| Lake Palas Tuzla | Palas Tuzla Gölü | 106\\xa0km2 | 15\\xa0m | Palas/Kayseri |\\n| Lake Akşehir | Akşehir Gölü | 105\\xa0km2 | | Akşehir in Konya, Afyonkarahisar |\\n| Lake Eber | Eber Gölü | 104\\xa0km2 | | Afyonkarahisar |\\n| Lake Erçek | Erçek Gölü | 98\\xa0km2 | | Van |\\n| Lake Hazar | Hazar Gölü | 86\\xa0km2 | | Elazığ |\\n| Lake Bafa | Bafa Gölü | 60\\xa0km2 | | Aydın, Muğla |\\n| Lake Köyceğiz | Köyceğiz Gölü | 52\\xa0km2 | | Köyceğiz in Muğla |\\n| Lake Işıklı | Işıklı Gölü | 49\\xa0km2 | | Denizli |\\n| Lake Nazik | Nazik Gölü | 48\\xa0km2 | | Bitlis |\\n| Lake Sapanca | Sapanca Gölü | 47\\xa0km2 | | Sakarya Province |\\n| Lake Salda | Salda Gölü | 45\\xa0km2 | 184\\xa0m | Burdur |\\n| Lake Yay | Yay Gölü | 37\\xa0km2 | | Kayseri |\\n| Lake Akyatan | Akyatan Gölü | 35\\xa0km2 | | Adana |\\n| Lake Balık | Balık Gölü | 34\\xa0km2 | | Doğubeyazıt in Ağrı |\\n| Lake Marmara | Marmara Gölü | 34\\xa0km2 | | Salihli, Gölmarmara in Manisa |\\n| Lake Çöl | Çöl Gölü | 32\\xa0km2 | | Ankara |\\n| Lake Durusu (Lake Terkos) | Durusu Gölü | 25\\xa0km2 | | İstanbul |\\n| Lake Karine | Karine Gölü | 24\\xa0km2 | | |\\n| Lake Tuzla | Tuzla Gölü | 23\\xa0km2 | | Tuzla |\\n| Lake Küçükçekmece | Küçükçekmece Gölü | 16\\xa0km2 | | Küçükçekmece, İstanbul |\\n| Lake Yaraşlı | Yaraşlı Gölü | 16\\xa0km2 | | Burdur |\\n| Lake Haçlı | Haçlı Gölü | 16\\xa0km2 | | Muş |\\n| Lake Seyfe | Seyfe Gölü | 15\\xa0km2 | | Kırşehir |\\n| Lake Akyayan | Akyayan Gölü | 15\\xa0km2 | | |\\n| Lake Hozapin | Hozapin Gölü | 14\\xa0km2 | | |\\n| Lake Arin | Arin Gölü | 13\\xa0km2 | | |\\n| Lake Nemrut | Nemrut Gölü | 12\\xa0km2 | | Bitlis Province |\\n| Lake Balık | Balık Gölü | 12\\xa0km2 | | |\\n| Lake Büyükçekmece | Büyükçekmece Gölü | 11\\xa0km2 | | Büyükçekmece, Istanbul |\\n| Lake Boluk | Boluk Gölü | 11\\xa0km2 | | |\\n| Lake Akdoğan | Akdoğan Gölü | 11\\xa0km2 | | |\\n| Lake Çavuşlu | Çavuşlu Gölü | 9\\xa0km2 | | |\\n| Lake Düden | Düden Gölü | 8\\xa0km2 | | |\\n| Lake Gala | Gala Gölü | 8\\xa0km2 | | Edirne |\\n| Lake Karataş | Karataş Gölü | 6\\xa0km2 | | |\\n| Lake Mogan | Mogan Gölü | 6\\xa0km2 | | Ankara |\\n| Paradeniz | Paradeniz | 4\\xa0km2 | | Mersin |\\n| Lake Eymir | Eymir Gölü | 1.8\\xa0km2 | | Ankara |\\n| Lake Abant | Abant Gölü | 1.28\\xa0km2 | 18\\xa0m | Bolu |\\n| Lake Gölcük | Gölcük Gölü | 1\\xa0km2 | | İzmir |\\n',\n",
       " 'input_txt': 'question: which is deeper, lake tuz or lake palas tuzla? table: | Name in English | Name in Turkish | Area (km2) | Depth | Location (districts and/or provinces) |\\n| Lake Van | Van Gölü | 3755\\xa0km2 | 451\\xa0m | Van, Bitlis |\\n| Lake Tuz | Tuz Gölü | 1500\\xa0km2 | 2\\xa0m | Aksaray, Ankara, Konya |\\n| Lake Beyşehir | Beyşehir Gölü | 656\\xa0km2 | 10\\xa0m | Beyşehir in Konya, Isparta |\\n| Lake Eğirdir | Eğirdir Gölü | 482\\xa0km2 | | Isparta |\\n| Lake İznik | İznik Gölü | 308\\xa0km2 | | İznik in Bursa, Yalova |\\n| Lake Burdur | Burdur Gölü | 200\\xa0km2 | | Burdur, Isparta |\\n| Lake Manyas | Manyas Gölü | 166\\xa0km2 | | Balıkesir |\\n| Lake Acıgöl | Acıgöl | 153\\xa0km2 | | Denizli, Afyonkarahisar |\\n| Lake Uluabat | Uluabat Gölü | 134\\xa0km2 | 1–2\\xa0m | Bursa |\\n| Lake Çıldır | Çıldır Gölü | 115\\xa0km2 | | Ardahan, Kars |\\n| Lake Palas Tuzla | Palas Tuzla Gölü | 106\\xa0km2 | 15\\xa0m | Palas/Kayseri |\\n| Lake Akşehir | Akşehir Gölü | 105\\xa0km2 | | Akşehir in Konya, Afyonkarahisar |\\n| Lake Eber | Eber Gölü | 104\\xa0km2 | | Afyonkarahisar |\\n| Lake Erçek | Erçek Gölü | 98\\xa0km2 | | Van |\\n| Lake Hazar | Hazar Gölü | 86\\xa0km2 | | Elazığ |\\n| Lake Bafa | Bafa Gölü | 60\\xa0km2 | | Aydın, Muğla |\\n| Lake Köyceğiz | Köyceğiz Gölü | 52\\xa0km2 | | Köyceğiz in Muğla |\\n| Lake Işıklı | Işıklı Gölü | 49\\xa0km2 | | Denizli |\\n| Lake Nazik | Nazik Gölü | 48\\xa0km2 | | Bitlis |\\n| Lake Sapanca | Sapanca Gölü | 47\\xa0km2 | | Sakarya Province |\\n| Lake Salda | Salda Gölü | 45\\xa0km2 | 184\\xa0m | Burdur |\\n| Lake Yay | Yay Gölü | 37\\xa0km2 | | Kayseri |\\n| Lake Akyatan | Akyatan Gölü | 35\\xa0km2 | | Adana |\\n| Lake Balık | Balık Gölü | 34\\xa0km2 | | Doğubeyazıt in Ağrı |\\n| Lake Marmara | Marmara Gölü | 34\\xa0km2 | | Salihli, Gölmarmara in Manisa |\\n| Lake Çöl | Çöl Gölü | 32\\xa0km2 | | Ankara |\\n| Lake Durusu (Lake Terkos) | Durusu Gölü | 25\\xa0km2 | | İstanbul |\\n| Lake Karine | Karine Gölü | 24\\xa0km2 | | |\\n| Lake Tuzla | Tuzla Gölü | 23\\xa0km2 | | Tuzla |\\n| Lake Küçükçekmece | Küçükçekmece Gölü | 16\\xa0km2 | | Küçükçekmece, İstanbul |\\n| Lake Yaraşlı | Yaraşlı Gölü | 16\\xa0km2 | | Burdur |\\n| Lake Haçlı | Haçlı Gölü | 16\\xa0km2 | | Muş |\\n| Lake Seyfe | Seyfe Gölü | 15\\xa0km2 | | Kırşehir |\\n| Lake Akyayan | Akyayan Gölü | 15\\xa0km2 | | |\\n| Lake Hozapin | Hozapin Gölü | 14\\xa0km2 | | |\\n| Lake Arin | Arin Gölü | 13\\xa0km2 | | |\\n| Lake Nemrut | Nemrut Gölü | 12\\xa0km2 | | Bitlis Province |\\n| Lake Balık | Balık Gölü | 12\\xa0km2 | | |\\n| Lake Büyükçekmece | Büyükçekmece Gölü | 11\\xa0km2 | | Büyükçekmece, Istanbul |\\n| Lake Boluk | Boluk Gölü | 11\\xa0km2 | | |\\n| Lake Akdoğan | Akdoğan Gölü | 11\\xa0km2 | | |\\n| Lake Çavuşlu | Çavuşlu Gölü | 9\\xa0km2 | | |\\n| Lake Düden | Düden Gölü | 8\\xa0km2 | | |\\n| Lake Gala | Gala Gölü | 8\\xa0km2 | | Edirne |\\n| Lake Karataş | Karataş Gölü | 6\\xa0km2 | | |\\n| Lake Mogan | Mogan Gölü | 6\\xa0km2 | | Ankara |\\n| Paradeniz | Paradeniz | 4\\xa0km2 | | Mersin |\\n| Lake Eymir | Eymir Gölü | 1.8\\xa0km2 | | Ankara |\\n| Lake Abant | Abant Gölü | 1.28\\xa0km2 | 18\\xa0m | Bolu |\\n| Lake Gölcük | Gölcük Gölü | 1\\xa0km2 | | İzmir |\\n',\n",
       " 'target_ids': tensor([ 2154, 14294,     7,  2740,   172,   521,     1,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vai pfv loss: 4.6575140953063965\n",
      "0 - Decoded: ['6,859', 'Chantelle and Steve', 'Los Angeles Rams', 'Foolad ad 5-2 on penalties 5-2 on penalties', 'United States s s s s s s s', '27 Jul', 'Chicago', '20,000', '24', '2', '7', 'Eskaton 001', 'SGD 2,400', 'H. C. McNeile', '2 s s s s s s s']\n",
      "0 - Real: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7', '6', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "Vai pfv loss: 1.6400213241577148\n",
      "1 - Decoded: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Fooladghlal Ahvaz', 'Canada', '04 Aug', 'Chicago', '72', '24', '2', '7', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "1 - Real: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7', '6', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "Vai pfv loss: 1.0090794563293457\n",
      "2 - Decoded: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad Esteghlal Ahvaz Foolad Foolad Fool', 'Canada', '07 Aug', 'Chicago', '72', '2', '2', '7', 'Coil Coil Coil Coil Coil Coil Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "2 - Real: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7', '6', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "Vai pfv loss: 0.6900229454040527\n",
      "3 - Decoded: ['6,859', 'Brad and Dale Brad and Dale Brad and Dale Brad and Dale', 'Los Angeles Rams', 'FooladEsteghlal Ahvaz Foolad Foolad', 'Canada (', '07 Aug', 'Chicago', '55', '5', '2', '7', 'Coil Coil Coil Coil Coil Coil Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "3 - Real: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7', '6', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "Vai pfv loss: 0.39509671926498413\n",
      "4 - Decoded: ['6,859', 'Brad and Dale Brad and Dale Brad and Dale', 'Los Angeles Rams Los Angeles Rams Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada', '07 Aug', 'Buffalo', '55', '5', '2', '7', 'Coil Coil Coil Coil Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "4 - Real: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7', '6', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "Vai pfv loss: 0.2860644459724426\n",
      "5 - Decoded: ['6,859', 'Brad and Dale', 'Los Angeles Rams Los Angeles Rams Los Angeles Rams Los Angeles Rams', 'Foolad|Esteghlal Ahvazesegh', 'Canada|Austria|Austria|Austria|Austria|Austri', '07 Aug', 'Buffalo', '1955', '5', '5', '7', 'Coil Coil Coil Coil Coil Coil Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "5 - Real: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7', '6', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "Vai pfv loss: 0.24859438836574554\n",
      "6 - Decoded: ['6,859', 'Brad and Dale', 'Los Angeles Rams Los Angeles Rams Los Angeles Rams Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|ItalyItaly', '07 Aug', 'Buffalo', '45', '6', '5|||||||||', '7', 'Coil Coil Coil Coil Coil Coil Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "6 - Real: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7', '6', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "Vai pfv loss: 0.21097135543823242\n",
      "7 - Decoded: ['6,859', 'Brad and Dale', 'Los Angeles Rams Los Angeles Rams Los Angeles Rams Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|ItalyItalyIta', '07 Aug', 'Buffalo', '45', '6', '7|||||||||||||', '6', 'Coil Coil Coil Coil Coil Coil Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "7 - Real: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7', '6', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "Vai pfv loss: 0.08517932891845703\n",
      "8 - Decoded: ['6,859', 'Brad and Dale', 'Los Angeles Rams Los Angeles Rams Los Angeles Rams Los Angeles Rams', 'Foolad|Esteghl Ahvaz|Rah Ahan', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7||||||||||||||', '6', 'Coil Coil Coil Coil Coil Coil Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "8 - Real: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7', '6', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "Vai pfv loss: 0.10224920511245728\n",
      "9 - Decoded: ['6,859|||||||||||||||', 'Brad and Dale', 'Los Angeles Rams Los Angeles Rams Los Angeles Rams Los Angeles Rams', 'Foolad|Esteghl Ahvaz|Rah Ahan', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7|||||||||||||', '6', 'Coil Coil Coil Coil Coil Coil Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "9 - Real: ['6,859', 'Brad and Dale', 'Los Angeles Rams', 'Foolad|Esteghlal Ahvaz', 'Canada|United States|Austria|Italy', '07 Aug', 'Buffalo', '45', '6', '7', '6', 'Coil', 'Ravi Sandiran', 'H. C. McNeile', '2']\n",
      "9 - Question: ['what was the average number in attendance against portland lumberjax on january 9, 2009?', 'in week 3 the winning couple in guest bedroom 2 was kyal and kara but the chumps were', 'which was the last team played?', 'each of these teams both scored 4 goals', 'which countries won the more than five silver medals?', 'which game was attended by more people, august 7 or september 22?', 'which cities population will grow the least?', 'how many total days was at&t on strike?', 'how many games were played in la?', 'how many seasons did the teams score above 40 points?', 'what are the number of channels owned by the government?', 'which artists had more then 6 release titles', 'which is the above tengku hadzali shah', \"which author had all of his titles' first edition lengths above 300 pp?\", 'how many matches ended in an aggregate tie record?']\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'features' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2b3d1b24e95c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-2b3d1b24e95c>\u001b[0m in \u001b[0;36msample\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#         print(f\"Question: {samples['question'][idx]}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m#     print(f\"Questions: {samples['question']}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'features' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def sample():\n",
    "    \n",
    "    decoder = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "#     encoder = T5EncoderModel.from_pretrained('t5-base')\n",
    "\n",
    "    encoder = decoder.get_encoder()\n",
    "    \n",
    "    dl = DataLoader(ds, batch_size=15, shuffle=True)\n",
    "    samples = next(iter(dl))\n",
    "        \n",
    "    opt = optim.Adam(itertools.chain(encoder.parameters(), decoder.parameters()), lr=5e-4)\n",
    "    \n",
    "    for epo in range(10):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        \n",
    "        enc_out = encoder(input_ids=samples['input_ids'],\n",
    "                          attention_mask=samples['input_attn_mask'],\n",
    "                          output_attentions=True)\n",
    "        \n",
    "        hidden = enc_out.last_hidden_state\n",
    "        \n",
    "        loss = decoder(encoder_outputs=(hidden, ), \n",
    "                       labels=samples['target_ids']).loss\n",
    "        \n",
    "        print(f\"Vai pfv loss: {loss}\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            \n",
    "            enc_out = encoder(input_ids=samples['input_ids'],\n",
    "                     attention_mask=samples['input_attn_mask'],\n",
    "                             output_attentions=True)\n",
    "        \n",
    "            encoder_hidden_states = enc_out.last_hidden_state\n",
    "            \n",
    "\n",
    "            B = encoder_hidden_states.size(0)\n",
    "    \n",
    "            decoded_ids = torch.full((B, 1),\n",
    "                                 decoder.config.decoder_start_token_id,\n",
    "                                 dtype=torch.long)\n",
    "            \n",
    "            for step in range(20):\n",
    "                outputs = decoder(decoder_input_ids=decoded_ids,\n",
    "                                  encoder_outputs=(encoder_hidden_states,),\n",
    "                                  return_dict=True)\n",
    "#                 outputs = decoder(decover_input_embeds)\n",
    "                logits = outputs[\"logits\"]\n",
    "\n",
    "                next_token_logits = logits[:, -1, :]\n",
    "\n",
    "                # Greedy decoding\n",
    "                next_token_id = next_token_logits.argmax(1).unsqueeze(-1)\n",
    "\n",
    "                # Check if output is end of senquence for all batches\n",
    "                if torch.eq(next_token_id[:, -1], tokenizer.eos_token_id).all():\n",
    "                    break\n",
    "\n",
    "                # Concatenate past ids with new id, keeping batch dimension\n",
    "                decoded_ids = torch.cat([decoded_ids, next_token_id], dim=-1)\n",
    "            \n",
    "            print(f\"{epo} - Decoded: {tokenizer.batch_decode(decoded_ids, skip_special_tokens=True)}\")\n",
    "            print(f\"{epo} - Real: {samples['answer']}\")\n",
    "    \n",
    "    print(f\"{epo} - Question: {samples['question']}\")\n",
    "    print(f\"{epo} - Question: {samples['table_str']}\")\n",
    "#     for elem in enumerate(samples['table_img']):\n",
    "#         img = np.transpose(elem.cpu().numpy(), (1, 2, 0))\n",
    "#         plt.figure(figsize=(15, 15))\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()\n",
    "#         print(f\"Question: {samples['question'][idx]}\")\n",
    "#     print(f\"Questions: {samples['question']}\")\n",
    "    del encoder, decoder, hidden, enc_out, opt\n",
    "\n",
    "sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### target.input_ids.shape, target.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = encoder(input_ids=target.input_ids, attention_mask=target.attention_mask, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape, a.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.cat([v.view(1, -1),a.last_hidden_state.view(1, -1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenizer.encode_plus(\"204\",\n",
    "                                       padding='max_length',\n",
    "                                       truncation=True,\n",
    "                                       max_length=128,\n",
    "                                       return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder(encoder_outputs=(t.permute(0, 2, 1), ), labels=labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.config.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajustado = embedding_extractor(img_tensor.unsqueeze(0)).permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajustado = ajustado.view(1, -1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajustado = ajustado.view(1, -1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajustado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajustado.permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nn.Linear(1575, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = l(ajustado.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/WikiTableQuestions/csv/200-csv/1.table'\n",
    "f = open(p, 'r')\n",
    "x = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = x.replace(\"\\t\", \" \\t \")\n",
    "# x = x.replace(\"\\\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "x = re.sub(\"\\s\\s+\" , \" \", x)\n",
    "x = x.replace(\"\\n\", \" \\n \")\n",
    "# x.replace(/  +/g, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
