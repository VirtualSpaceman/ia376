{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 14 18:15:12 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000     On   | 00000000:89:00.0 Off |                  Off |\n",
      "| 33%   27C    P8     6W / 230W |   3060MiB / 16095MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questionId': 338,\n",
       " 'question': 'what is the contact person name mentioned in letter?',\n",
       " 'image': 'documents/xnbl0037_1.png',\n",
       " 'docId': 279,\n",
       " 'ucsf_document_id': 'xnbl0037',\n",
       " 'ucsf_document_page_no': '1',\n",
       " 'answers': ['P. Carter', 'p. carter'],\n",
       " 'data_split': 'train'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/docvqa/train/train_v1.0.json'\n",
    "j = json.load(open(path))\n",
    "j['data'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_string = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Abstracts DocVQA\n",
    "'''\n",
    "\n",
    "\n",
    "class DocVQA(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 mode: str,\n",
    "                 transform: object = None,\n",
    "                 seq_len: int = 128):\n",
    "        '''\n",
    "        mode: one of train, val and test.\n",
    "        transform: transforms to be applied to the document image if applicable.\n",
    "        seq_len: maximum sequence len of encoded tokens.\n",
    "\n",
    "        returns:\n",
    "            dict:\n",
    "                document: transformed document image.\n",
    "                input_tokens: tokenized text contained in the document.\n",
    "                input_text: text contained in the document.\n",
    "                bboxes: bounding boxes for each OCR detection in the document, on the format [tl_col, tl_row, br_col, br_row].\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "        with open(f\"/docvqa/{mode}/{mode}_v1.0.json\", 'r') as data_json_file:\n",
    "            self.data_json = json.load(data_json_file)\n",
    "\n",
    "        self.folder = f\"/docvqa/{mode}\"\n",
    "        self.transform = transform\n",
    "        self.seq_len = seq_len\n",
    "        self.mode = mode\n",
    "\n",
    "        print(f\"{self.mode} DocVQA folder {self.folder} tokenizer {tokenizer.__class__.__name__} transform {self.transform} seq_len {self.seq_len}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_json[\"data\"])\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        data = self.data_json[\"data\"][i]\n",
    "        \n",
    "        document = Image.open(os.path.join(self.folder, data[\"image\"])).convert(\"RGB\")\n",
    "        \n",
    "        question_text = data['question']\n",
    "        answer_text = np.random.choice(data.get('answers', [\"N/A\"]))\n",
    "        \n",
    "        target_text = \"question: \" + question_text.strip() + \" answer: \" + answer_text.strip()\n",
    "        \n",
    "\n",
    "#         target_ids = self.tokenizer.encode(target_text,\n",
    "#                                        padding='max_length',\n",
    "#                                        truncation=True,\n",
    "#                                        max_length=self.seq_len,\n",
    "#                                        return_tensors='pt')[0]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            document = self.transform(document)\n",
    "\n",
    "        return {\"document\": document,\n",
    "                \"question\": question_text,\n",
    "                \"answer\": answer_text,\n",
    "                \"target_text\": target_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import AdamW\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.base import Callback\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "class CaptioningModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # EfficientNet image encoder.\n",
    "        self.encoder = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "        self.decoder = T5ForConditionalGeneration.from_pretrained(tokenizer_string)\n",
    "        # for par in self.decoder.base_model.parameters():\n",
    "        #     par.requires_grad = False\n",
    "        \n",
    "        # Bridge convolution layer between efficientnet and transformer formats.\n",
    "        self.bridge = nn.Conv2d(in_channels=112, out_channels=self.decoder.config.d_model, kernel_size=1)\n",
    "        \n",
    "    def _embeds_forward(self, img):\n",
    "\n",
    "        # Retrieve the features from the last layer.\n",
    "        # https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/efficientnet_pytorch/model.py#L231\n",
    "        #\n",
    "        # >>> img = torch.ones((1, 3, 256, 256))\n",
    "        # >>> encoder.extract_features(img).shape\n",
    "        # torch.Size([1, 1280, 8, 8])\n",
    "        # features = self.encoder.extract_features(img)\n",
    "        features = self.encoder.extract_endpoints(img)[\"reduction_4\"] \n",
    "\n",
    "        # print(features.shape)\n",
    "\n",
    "        # Compute the bridge convolution, it should reformat the inputs to feed the transformer network.\n",
    "        features = self.bridge(features)\n",
    "        \n",
    "        # Reshape the output to match the embedding dimension of the encoder with 64 tokens.\n",
    "        inputs_embeds = features \\\n",
    "            .permute(0, 2, 3, 1) \\\n",
    "            .reshape(features.shape[0], -1, self.decoder.config.d_model)\n",
    "\n",
    "        return inputs_embeds\n",
    "\n",
    "    def forward(self, img=None, inputs_embeds=None, decoder_input_ids=None, labels=None):\n",
    "\n",
    "        # Pass efficientnet hidden states as embeddings for the transformer encoder input.\n",
    "        inputs_embeds = self._embeds_forward(img) if inputs_embeds is None else inputs_embeds\n",
    "\n",
    "        return self.decoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_input_ids=decoder_input_ids, \n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    def generate(self, img, max_len=max_length):\n",
    "\n",
    "        # We need to implement our own generate loop as transformers doesn't accept \n",
    "        # precomputed embeddings on the generate method.\n",
    "        # Issue: https://github.com/huggingface/transformers/issues/7626\n",
    "        # Precompute embeddings to speedup generation as they don't change.\n",
    "        inputs_embeds = self._embeds_forward(img)\n",
    "        \n",
    "        decoder_input_ids = torch.full(\n",
    "            (1, 1), self.decoder.config.decoder_start_token_id, dtype=torch.long, device=img.device\n",
    "        )\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            with torch.no_grad():\n",
    "                output = self.forward(decoder_input_ids=decoder_input_ids, \n",
    "                                      inputs_embeds=inputs_embeds)\n",
    "\n",
    "                logits = output[0]\n",
    "                next_token_logits = logits[:, -1, :]\n",
    "                next_token_id = next_token_logits.argmax(1).unsqueeze(-1)\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, next_token_id], dim=-1).to(img.device)\n",
    "\n",
    "                if torch.eq(next_token_id[:, -1], self.decoder.config.eos_token_id).all():\n",
    "                    break\n",
    "\n",
    "        return decoder_input_ids\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, targets, raw_text = batch\n",
    "        output = self(img, labels=targets)\n",
    "        return output[0]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, targets, raw_text = batch\n",
    "        \n",
    "        tokens = [self.generate(im.view((1,) + im.shape))[0].cpu() for im in img]\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            loss_val = self(img, labels=targets)[0].item()\n",
    "\n",
    "        return (tokens, raw_text, img, loss_val)\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        \n",
    "        validation_step_outputs = list(validation_step_outputs)\n",
    "    \n",
    "        tokens_batch = [t for out in validation_step_outputs for t in out[0]]\n",
    "        reference_batch = [r for out in validation_step_outputs for r in out[1]]\n",
    "        # img_batch = [i for out in validation_step_outputs for i in out[2]]\n",
    "        hist_loss_val = np.mean([out[3] for out in validation_step_outputs])\n",
    "        \n",
    "        generated_batch = tokenizer.batch_decode(tokens_batch)\n",
    "        \n",
    "\n",
    "        F1_val = np.mean([metrics.compute_f1(gold, pred) for gold, pred in zip(reference_batch,\n",
    "                                                                    generated_batch)])\n",
    "\n",
    "\n",
    "        exact_val = np.mean([metrics.compute_exact(gold, pred) for gold, pred in zip(reference_batch,\n",
    "                                                                            generated_batch)])\n",
    "        \n",
    "        self.log(\"loss_val\", hist_loss_val, prog_bar=True)\n",
    "        self.log(\"exact_val\", exact_val, prog_bar=True)\n",
    "        self.log(\"F1_val\", F1_val, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        img, targets, raw_text = batch\n",
    "        \n",
    "        tokens = [self.generate(im.view((1,) + im.shape))[0].cpu() for im in img]\n",
    "        \n",
    "        return (tokens, raw_text, img.cpu())\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        test_step_outputs = list(test_step_outputs)\n",
    "    \n",
    "        tokens_batch = [t for out in test_step_outputs for t in out[0]]\n",
    "        reference_batch = [r for out in test_step_outputs for r in out[1]]\n",
    "        \n",
    "        generated_batch = tokenizer.batch_decode(tokens_batch)\n",
    "        \n",
    "        F1_test = np.mean([metrics.compute_f1(gold, pred) for gold, pred in zip(reference_batch,\n",
    "                                                                    generated_batch)])\n",
    "\n",
    "\n",
    "        exact_test = np.mean([metrics.compute_exact(gold, pred) for gold, pred in zip(reference_batch,\n",
    "                                                                            generated_batch)])\n",
    "        \n",
    "        self.log(\"exact_test\", exact_test, prog_bar=True)\n",
    "        self.log(\"F1_test\", F1_test, prog_bar=True)\n",
    "\n",
    "        return {\"exact_test\": exact_test,\n",
    "                \"F1_test\": F1_test}\n",
    "    \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentando, testando e salvando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train DocVQA folder /docvqa/train tokenizer T5Tokenizer transform Compose(\n",
      "    Resize(size=(700, 400), interpolation=PIL.Image.BILINEAR)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ") seq_len 128\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Input any (selected randomly) caption sample for every image. Useful for training.\n",
    "    \"\"\"\n",
    "    #lista de [imagem, token_ids, texto_original]\n",
    "    imgs = [r['document'].numpy() for r in batch]\n",
    "    texts = [r['target_text'] for r in batch]\n",
    "\n",
    "\n",
    "    tokens_ids = tokenizer.batch_encode_plus(texts,\n",
    "                                               truncation=True, \n",
    "                                               return_tensors=\"pt\", \n",
    "                                               padding=\"max_length\",\n",
    "                                               max_length=max_length)['input_ids']\n",
    "    \n",
    "    return (\n",
    "        torch.Tensor(imgs),\n",
    "        tokens_ids, \n",
    "        texts,\n",
    "    )\n",
    "\n",
    "\n",
    "val_test_transforms = transforms.Compose([transforms.Resize((700, 400)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std =[0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "\n",
    "\n",
    "#cria os datasets necessáios\n",
    "dataset_train = DocVQA(mode='train', transform=val_test_transforms)\n",
    "\n",
    "batch = 3\n",
    "num_workers = 8\n",
    "\n",
    "debug_train = DataLoader(dataset_train, \n",
    "                                batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                pin_memory=True,\n",
    "                                num_workers=num_workers,\n",
    "                                collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train DocVQA folder /docvqa/train tokenizer T5Tokenizer transform Compose(\n",
      "    Resize(size=(100, 100), interpolation=PIL.Image.BILINEAR)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ") seq_len 128\n",
      "val DocVQA folder /docvqa/val tokenizer T5Tokenizer transform Compose(\n",
      "    Resize(size=(100, 100), interpolation=PIL.Image.BILINEAR)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ") seq_len 128\n",
      "test DocVQA folder /docvqa/test tokenizer T5Tokenizer transform Compose(\n",
      "    Resize(size=(100, 100), interpolation=PIL.Image.BILINEAR)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ") seq_len 128\n",
      "None\n",
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-small were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                       | Params\n",
      "-------------------------------------------------------\n",
      "0 | encoder | EfficientNet               | 5.3 M \n",
      "1 | decoder | T5ForConditionalGeneration | 60.5 M\n",
      "2 | bridge  | Conv2d                     | 57.9 K\n",
      "-------------------------------------------------------\n",
      "65.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "65.9 M    Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41fc36ac6134b68839162d17bb338a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27262afdd2824513ba756963586d5d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([transforms.Resize((700, 400)),\n",
    "                                    transforms.ColorJitter(0.2, 0.3, 0.3, 0.4),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std =[0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "\n",
    "\n",
    "val_test_transforms = transforms.Compose([transforms.Resize((700, 400)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std =[0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "\n",
    "\n",
    "#cria os datasets necessáios\n",
    "dataset_train = DocVQA(mode='train', transform=train_transforms)\n",
    "dataset_val = DocVQA(mode='val', transform=val_test_transforms)\n",
    "dataset_test = DocVQA(mode='test', transform=val_test_transforms)\n",
    "\n",
    "batch = 4\n",
    "\n",
    "num_workers = 8\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                                batch_size=batch,\n",
    "                                shuffle=True,\n",
    "                                pin_memory=True,\n",
    "                                num_workers=num_workers,\n",
    "                                collate_fn=collate_fn)\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, \n",
    "                            batch_size=batch,\n",
    "                            shuffle=False,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=num_workers,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, \n",
    "                            batch_size=batch,\n",
    "                            shuffle=False,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=num_workers,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "path_to_checkpoint= '/logs/captioning-epoch=9-loss_val=15.81-docvqa80.ckpt'\n",
    "\n",
    "if not os.path.isfile(path_to_checkpoint):\n",
    "    path_to_checkpoint = None\n",
    "else:\n",
    "    print(f\"Logging from: {path_to_checkpoint}\")\n",
    "\n",
    "# Log results to CSV (so we can plot them later).\n",
    "logger = pl.loggers.csv_logs.CSVLogger(f\"./logs\", name=\"vqa-generation\")\n",
    "\n",
    "\n",
    "# Checkpoint the best model.\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    prefix=\"captioning\",\n",
    "    filepath=\"logs/{epoch}-{loss_val:.2f}-docvqa\"+str(max_length), \n",
    "    monitor=\"F1_val\", \n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "print(path_to_checkpoint)\n",
    "model = CaptioningModule()\n",
    "model_trainer = pl.Trainer(gpus=1, \n",
    "                           max_epochs=25,\n",
    "                           callbacks=[checkpoint_callback],\n",
    "                           logger=logger,\n",
    "                           resume_from_checkpoint=path_to_checkpoint,\n",
    "                           accumulate_grad_batches=2,\n",
    "                           check_val_every_n_epoch=5)\n",
    "model_trainer.fit(model, dataloader_train, dataloader_val)\n",
    "\n",
    "print(\"Test Stage....\")\n",
    "\n",
    "model_trainer.test(model, dataloader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
