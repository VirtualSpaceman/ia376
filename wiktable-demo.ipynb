{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5EncoderModel\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from Datasets import Modes, WikiTable\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "ds = WikiTable(Modes.TRAIN, tokenizer=tokenizer, img_shape=(500, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pergunta = \"what was the last year where this team was a part of the usl a-league?\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "# target = tokenizer.encode_plus(pergunta,\n",
    "#                                        padding='max_length',\n",
    "#                                        truncation=True,\n",
    "#                                        max_length=128,\n",
    "#                                        return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = T5EncoderModel.from_pretrained('t5-base')\n",
    "# decoder = decoder = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.config.d_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                   nn.BatchNorm2d(out_channels),\n",
    "                                   nn.LeakyReLU(),\n",
    "                                   nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "                                   nn.BatchNorm2d(out_channels),\n",
    "                                   nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "embedding_extractor = nn.Sequential(ConvBlock(3, 16),\n",
    "                                     ConvBlock(16, 64),\n",
    "                                     ConvBlock(64, 256),\n",
    "                                     ConvBlock(256, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ds[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table_img': tensor([[[-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          ...,\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059]],\n",
       " \n",
       "         [[-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          ...,\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059]],\n",
       " \n",
       "         [[-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          ...,\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059],\n",
       "          [-0.9059, -0.9059, -0.9059,  ..., -0.9059, -0.9059, -0.9059]]]),\n",
       " 'question': 'which is deeper, lake tuz or lake palas tuzla?',\n",
       " 'question_ids': tensor([   84,    19,  7231,     6,  6957,     3,    17,    76,   172,    42,\n",
       "          6957,     3, 13878,     7,     3,    17,    76,   172,   521,    58,\n",
       "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'question_attn_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'answer': 'Lake Palas Tuzla',\n",
       " 'target_ids': tensor([ 2154, 14294,     7,  2740,   172,   521,     1,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = embedding_extractor(ds[0]['table_img'].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-base were not used when initializing T5EncoderModel: ['decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vai pfv: 10.959039688110352\n",
      "Decoded: ['a a solitary a single a single.']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 10.099553108215332\n",
      "Decoded: ['a a solitary a single a single.']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 8.624160766601562\n",
      "Decoded: ['a a solitary a single a single a']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 9.869156837463379\n",
      "Decoded: ['a a solitary a single a single a']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 7.795677185058594\n",
      "Decoded: ['a a solitary a single a single a']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 8.571985244750977\n",
      "Decoded: ['a a a solitary a single a single']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 6.2537150382995605\n",
      "Decoded: ['a a a solitary a single a single']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 7.2113261222839355\n",
      "Decoded: ['a a a solitary a single a single']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 6.357486724853516\n",
      "Decoded: ['a a a solitary a single a single']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 5.1611328125\n",
      "Decoded: ['a a a sandbox a sandbox']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 5.619405269622803\n",
      "Decoded: ['']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 4.168094635009766\n",
      "Decoded: ['']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 3.6329901218414307\n",
      "Decoded: ['']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 3.4874203205108643\n",
      "Decoded: ['']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 3.2448246479034424\n",
      "Decoded: ['']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 2.4792730808258057\n",
      "Decoded: ['']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 2.1399447917938232\n",
      "Decoded: ['Die wichtigsten Industrie- und Handelsplätze sind die sterreichische Inseln und die']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 2.1754233837127686\n",
      "Decoded: ['Die wichtigsten Industrie- und Handelsplätze sind die rmelkanal, die']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 1.8944511413574219\n",
      "Decoded: ['Lake Huron - Lake Huron - Lake Huron - Lake Huron -']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 1.4821045398712158\n",
      "Decoded: ['Lake Huron - Lake Huron - Lake Huron - Lake Huron -']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 1.130665898323059\n",
      "Decoded: ['Lake Huron - Lake Huron - Lake Huron - Lake Huron -']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 0.7129525542259216\n",
      "Decoded: ['Lake Palas Tuzla']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 0.9223710298538208\n",
      "Decoded: ['Lake Palas Tuzla']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 0.47125959396362305\n",
      "Decoded: ['Lake Palas Tuzla']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 0.35897621512413025\n",
      "Decoded: ['Lake Palas Tuzla']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 0.2579798102378845\n",
      "Decoded: ['Lake Palas Tuzla']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 0.06046125665307045\n",
      "Decoded: ['Lake Palas Tuzla']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 0.05888713151216507\n",
      "Decoded: ['Lake Palas Tuzla']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 0.07522428780794144\n",
      "Decoded: ['Lake Palas Tuzla']\n",
      "Real: Lake Palas Tuzla\n",
      "Vai pfv: 0.09840918332338333\n",
      "Decoded: ['Lake Palas Tuzla']\n",
      "Real: Lake Palas Tuzla\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sample():\n",
    "    encoder = T5EncoderModel.from_pretrained('t5-base')\n",
    "    decoder = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    sample = ds[5]\n",
    "    \n",
    "#     enc_out = encoder(input_ids=sample['question_ids'].unsqueeze(0),\n",
    "#                      attention_mask=sample['question_attn_mask'].unsqueeze(0))\n",
    "    \n",
    "#     hidden = enc_out.last_hidden_state\n",
    "    \n",
    "#     print(f\"hidden shp: {hidden.shape}\")\n",
    "    \n",
    "#     print(sample['question_ids'].shape, sample['question_attn_mask'].shape)\n",
    "    \n",
    "    #torch.Size([1, 768, 32, 32])\n",
    "#     features = embedding_extractor(ds[0]['table_img'].unsqueeze(0))\n",
    "    \n",
    "    #torch.Size([1, 32, 32, 768]) -> torch.Size([1, 32*32, 768])\n",
    "#     features = features.permute(0, 2, 3, 1).view(1, -1, decoder.config.d_model)\n",
    "    \n",
    "#     torch.Size([1, 1152, 768])\n",
    "#     features = torch.cat([features, hidden], dim =1)\n",
    "    \n",
    "#     print(f\"feat shape: {features.shape}\")\n",
    "    \n",
    "    #torch.Size([1, 768, 1152])\n",
    "#     features = features.permute(0, 2, 1)\n",
    "    \n",
    "    linear = nn.Linear(1152, 128)\n",
    "    \n",
    "    #torch.Size([1, 768, 128])\n",
    "#     proj = linear(features)\n",
    "    \n",
    "    #hidden state\n",
    "    #torch.Size([1, 128, 728])\n",
    "#     proj = proj.permute(0, 2, 1)\n",
    "    \n",
    "    #batch size x seqlen x self.d_model\n",
    "#     print(f\"proj shape: {proj.shape}\")\n",
    "    \n",
    "    opt = optim.Adam(itertools.chain(encoder.parameters(), decoder.parameters(),\n",
    "                                    linear.parameters(), embedding_extractor.parameters()), lr=1e-4)\n",
    "    \n",
    "    for _ in range(30):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        \n",
    "        enc_out = encoder(input_ids=sample['question_ids'].unsqueeze(0),\n",
    "                     attention_mask=sample['question_attn_mask'].unsqueeze(0))\n",
    "        \n",
    "        hidden = enc_out.last_hidden_state\n",
    "        features = embedding_extractor(sample['table_img'].unsqueeze(0))\n",
    "        features = features.permute(0, 2, 3, 1).view(1, -1, decoder.config.d_model)\n",
    "        features = torch.cat([features, hidden], dim =1)\n",
    "        features = features.permute(0, 2, 1)\n",
    "        \n",
    "        proj = linear(features)\n",
    "        proj = proj.permute(0, 2, 1)\n",
    "        \n",
    "        loss = decoder(encoder_outputs=(proj, ), \n",
    "                       labels=sample['target_ids'].unsqueeze(0)).loss\n",
    "        \n",
    "        print(f\"Vai pfv: {loss}\")\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            embedding_extractor.eval()\n",
    "        \n",
    "            \n",
    "            enc_out = encoder(input_ids=sample['question_ids'].unsqueeze(0),\n",
    "                     attention_mask=sample['question_attn_mask'].unsqueeze(0))\n",
    "        \n",
    "            encoder_hidden_states = enc_out.last_hidden_state\n",
    "            features = embedding_extractor(sample['table_img'].unsqueeze(0))\n",
    "            features = features.permute(0, 2, 3, 1).view(1, -1, decoder.config.d_model)\n",
    "            features = torch.cat([features, encoder_hidden_states], dim =1)\n",
    "            features = features.permute(0, 2, 1)\n",
    "\n",
    "            proj = linear(features)\n",
    "            \n",
    "            #-------------------------------------------------\n",
    "            encoder_hidden_states = proj.permute(0, 2, 1)\n",
    "#             print(f\"Hidden shp: {encoder_hidden_states.shape}\")\n",
    "            \n",
    "            decoded_ids = torch.full((1, 1),\n",
    "                                 decoder.config.decoder_start_token_id,\n",
    "                                 dtype=torch.long)\n",
    "            \n",
    "            for step in range(20):\n",
    "                outputs = decoder(decoder_input_ids=decoded_ids,\n",
    "                                  encoder_outputs=(encoder_hidden_states,),\n",
    "                                  return_dict=True)\n",
    "                logits = outputs[\"logits\"]\n",
    "\n",
    "                next_token_logits = logits[:, -1, :]\n",
    "\n",
    "                # Greedy decoding\n",
    "                next_token_id = next_token_logits.argmax(1).unsqueeze(-1)\n",
    "\n",
    "                # Check if output is end of senquence for all batches\n",
    "                if torch.eq(next_token_id[:, -1], tokenizer.eos_token_id).all():\n",
    "                    break\n",
    "\n",
    "                # Concatenate past ids with new id, keeping batch dimension\n",
    "                decoded_ids = torch.cat([decoded_ids, next_token_id], dim=-1)\n",
    "            \n",
    "            print(f\"Decoded: {tokenizer.batch_decode(decoded_ids, skip_special_tokens=True)}\")\n",
    "            print(f\"Real: {sample['answer']}\")\n",
    "            \n",
    "    del encoder, decoder, hidden, enc_out, features, linear, proj, opt\n",
    "\n",
    "sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### encoder = T5EncoderModel.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]), torch.Size([1, 128]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.input_ids.shape, target.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = encoder(input_ids=target.input_ids, attention_mask=target.attention_mask, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2139,  0.1358,  0.2902,  ..., -0.5094, -0.5432, -0.0968],\n",
       "         [-0.2147, -0.0238, -0.1447,  ..., -0.2354, -0.1443, -0.3103],\n",
       "         [-0.0410,  0.0897, -0.3892,  ..., -0.2407, -0.1751, -0.6424],\n",
       "         ...,\n",
       "         [-0.4393,  0.1653, -0.1272,  ...,  0.0248,  0.2123,  0.0096],\n",
       "         [-0.4393,  0.1653, -0.1272,  ...,  0.0248,  0.2123,  0.0096],\n",
       "         [-0.4393,  0.1653, -0.1272,  ...,  0.0248,  0.2123,  0.0096]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1536, 16, 40]), torch.Size([1, 128, 768]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape, a.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.cat([v.view(1, -1),a.last_hidden_state.view(1, -1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1081344])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokenizer.encode_plus(\"204\",\n",
    "                                       padding='max_length',\n",
    "                                       truncation=True,\n",
    "                                       max_length=128,\n",
    "                                       return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "decoder = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3917, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(encoder_outputs=(t.permute(0, 2, 1), ), labels=labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.config.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajustado = embedding_extractor(img_tensor.unsqueeze(0)).permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 63, 768])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajustado = ajustado.view(1, -1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajustado = ajustado.view(1, -1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1575, 768])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajustado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 1575])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ajustado.permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nn.Linear(1575, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = l(ajustado.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
